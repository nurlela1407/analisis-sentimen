{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87949bd-78ed-4f78-a0a7-a60d5ef17f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "\n",
    "from functools import partial\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17aee04-46c5-4577-88c9-bb55c4fb3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCES_LENGTH = 50 # Maximum kata pada kalimat\n",
    "MAX_NB_WORDS = 6000 # Vocabulary size\n",
    "EMBEDDING_DIM = 50 # Dimensions of Glove word vector kadang bisa juga 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3795f3-0596-4687-ba71-7f7ce3744bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/1/2021 16:48</td>\n",
       "      <td>theoholan</td>\n",
       "      <td>itu mah konflik rebutan tanah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/2/2021 1:38</td>\n",
       "      <td>knb_rmdhna</td>\n",
       "      <td>Karena ga ada urgensinya. Ga ada ancaman inva...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/2/2021 5:51</td>\n",
       "      <td>_afaz</td>\n",
       "      <td>ini valid Ã°ÂÂÂ¯ kemarin saya smpat nonton ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/2/2021 7:19</td>\n",
       "      <td>PolitikBRIN</td>\n",
       "      <td>Analisis yang dirumuskan dari Klaster Politik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/3/2021 14:12</td>\n",
       "      <td>KoloElang</td>\n",
       "      <td>Bisa tidak? Palestina taat secara kaffah dala...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       created_at  screen_name  \\\n",
       "0  5/1/2021 16:48    theoholan   \n",
       "1   5/2/2021 1:38   knb_rmdhna   \n",
       "2   5/2/2021 5:51        _afaz   \n",
       "3   5/2/2021 7:19  PolitikBRIN   \n",
       "4  5/3/2021 14:12    KoloElang   \n",
       "\n",
       "                                                text  Label  \n",
       "0                      itu mah konflik rebutan tanah      0  \n",
       "1   Karena ga ada urgensinya. Ga ada ancaman inva...      0  \n",
       "2   ini valid Ã°ÂÂÂ¯ kemarin saya smpat nonton ...      0  \n",
       "3   Analisis yang dirumuskan dari Klaster Politik...      0  \n",
       "4   Bisa tidak? Palestina taat secara kaffah dala...      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open dataset,save on dataframe\n",
    "df = pd.read_csv(\"dataset.csv\",encoding=\"ISO-8859-1\")\n",
    "\n",
    "# membuka data slang (data yang akan dibenarkan)\n",
    "with open('slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "    for line in file if line.strip())\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf6aa05-d63c-4e1b-bdd7-02f72ec2b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column created_at and screen_name\n",
    "df = df.drop(['created_at', 'screen_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15648f74-55d2-485a-86e1-71f1c6b17e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing dataset\n",
    "# fungsi case folding dan menghapus RT dan CC\n",
    "def casefoldingRemoveRt(text):\n",
    "     # menghapus RT (reetwet)\n",
    "    text = re.sub(r'^[RT]+', '', text) \n",
    "    # case folding (menjadi lower case)\n",
    "    text = text.lower()\n",
    "    # menghapus CC (carbon copy)\n",
    "    text = re.sub(r'^[cc]+', '', text)\n",
    "    return text\n",
    "\n",
    "# fungsi untuk menghapus noise\n",
    "def removeNoise(text): \n",
    "    # menghapus unicode\n",
    "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)','', text)\n",
    "    # menghapus emoticon\n",
    "    text = re.sub(r'[^\\x00-\\x7f]','',text)\n",
    "    # menghapus url\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text) \n",
    "    # menghapus # (biasanya utk menandai topik tertentu)\n",
    "    text = re.sub(r'#([^\\s]+)', '', text) \n",
    "    # menghapus \"@users\"\n",
    "    text = re.sub('@[^\\s]+','',text)\n",
    "    return text\n",
    "\n",
    "# mengganti kata2 slang menjadi lebih baku\n",
    "slang_words = sorted(slang_map, key=len, reverse=True) # longest first for regex\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "\n",
    "def removePunctuation(text): \n",
    "    # menghapus integers/numbers\n",
    "    text = ''.join([i for i in text if not i.isdigit()]) \n",
    "    # khusus untuk tanda baca ' (dirapatkan dengan text)\n",
    "    text = re.sub(\"'\",'',text)\n",
    "    # menghapus punctuation (tanda baca)\n",
    "    text = re.sub(r\"[^A-Za-z]+\",\" \",text)\n",
    "    # menghapus 1 karakter\n",
    "    text = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', text)\n",
    "    # menghapus whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb218d4-aeba-41e1-b395-e7e55957a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: casefoldingRemoveRt(x))\n",
    "df['text'] = df['text'].apply(lambda x: removeNoise(x))\n",
    "df['text'] = df['text'].apply(lambda x: replaceSlang(x))\n",
    "df['text'] = df['text'].apply(lambda x: removePunctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14ea7b21-5a7a-48d8-b7fb-138273cb28ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>itu mah konflik rebutan tanah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>karena tidak ada urgensinya tidak ada ancaman ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ini sah kemarin saya sempat nonton salah satu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analisis yang dirumuskan dari klaster politik ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bisa tidak palestina taat secara kaffah dalam ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Label\n",
       "0                      itu mah konflik rebutan tanah      0\n",
       "1  karena tidak ada urgensinya tidak ada ancaman ...      0\n",
       "2  ini sah kemarin saya sempat nonton salah satu ...      0\n",
       "3  analisis yang dirumuskan dari klaster politik ...      0\n",
       "4  bisa tidak palestina taat secara kaffah dalam ...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5856d-2d23-44a5-9116-b663c50fc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenizing\n",
    "# from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046dc76d-ba5a-4481-99b6-93ba8bfdc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakukan tokenizer terlebih dahulu\n",
    "# stop = pd.read_csv('stopword.txt', sep=\" \", header=None)\n",
    "# klu mau custome bisa menggukan tokenizing terlebih dahulu\n",
    "# def word_tokenize_wrapper(text):\n",
    "#     return word_tokenize(text)\n",
    "\n",
    "# df['text'] = df['text'].apply(word_tokenize_wrapper)\n",
    "# df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "455b5c40-60f2-4623-961d-4cd3b8563b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Word\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f6fa7f4-4abd-44fc-ab68-d122a0cdec39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               mah konflik rebutan tanah\n",
       "1       urgensinya tidak ada ancaman invasi militer ne...\n",
       "2       sah kemarin sempat nonton salah satu wawancara...\n",
       "3       analisis dirumuskan klaster politik luar neger...\n",
       "4       tidak palestina taat kaffah islam mengganti de...\n",
       "                              ...                        \n",
       "1642    satukan hati satukan tekad satulan langkah ber...\n",
       "1643    kok ngelihatnya gencatan senjata sengaja mengu...\n",
       "1644    mungkin bergembira mau merayakan gencatan senj...\n",
       "1645    lucu sih lihat orang jadi die hard fans konfli...\n",
       "1646        zionis deh langgar gencatan senjata fuck kamu\n",
       "Name: text, Length: 1647, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stopWord(text):\n",
    "    stop = stopword.remove(text)\n",
    "    return stop\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: stopWord(x))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8bb319f-9f1c-4ae0-bf01-828b11e8247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb02651-9769-42e9-969d-fbf37fcb23f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 mah konflik rebut tanah\n",
       "1       urgensi tidak ada ancam invasi militer negara ...\n",
       "2       sah kemarin sempat nonton salah satu wawancara...\n",
       "3       analisis rumus klaster politik luar negeri isu...\n",
       "4       tidak palestina taat kaffah islam ganti demokr...\n",
       "                              ...                        \n",
       "1642    satu hati satu tekad satulan langkah sama buru...\n",
       "1643    kok ngelihatnya gencat senjata sengaja ulur se...\n",
       "1644    mungkin gembira mau raya gencat senjata israel...\n",
       "1645    lucu sih lihat orang jadi die hard fans konfli...\n",
       "1646          zionis deh langgar gencat senjata fuck kamu\n",
       "Name: text, Length: 1647, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(text):\n",
    "    stream = stemmer.stem(text)\n",
    "    return stream\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: stemming(x))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef9e7df-ce2e-4738-b3cd-6bcf9feee032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete missing value \n",
    "nan_value = float('NaN')\n",
    "df.replace('', nan_value, inplace=True)\n",
    "df.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e83ecbc-29fd-4bed-8b52-b72919d7d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete data duplicate\n",
    "df.drop_duplicates(subset='text', keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "329cbdc7-3c2b-4a14-adbb-5b4cbcf413f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    660\n",
      "2    525\n",
      "1    366\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# menghitung Label sentimen (menampilkan tabel pembagian label sentimen)\n",
    "labels = df.Label\n",
    "labels_count = labels.value_counts()\n",
    "labels_count.plot(kind=\"bar\", x='Label', y='Jumlah', color='red')\n",
    "print(labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "748348b2-07ae-4738-bfa6-5ac8dc4484d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to CSV\n",
    "df.to_csv('stopword&stemming.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b642154-ff9d-4cf3-99a8-eea3246a6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to X as Feature\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.text)\n",
    "sequences = tokenizer.texts_to_sequences(df.text)\n",
    "# Banyak kata yang telah di tokenizer\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed59d8c-75ce-48fb-9e48-c01ecba5156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found %d unique words.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37a086-664a-4251-aa97-573d2c2f794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCES_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae952c-b972-476b-95cf-51370ea87a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to y as label\n",
    "y = df[\"Label\"]\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a20db-6ac8-4735-acd1-57fd0f733f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['0', '1', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea47a00-cdd2-40f4-a47f-062972627cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=20, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362a351-476b-4d01-84f1-6a2ebcd1df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan dataset yg sudah di split agar bisa di panggil kembali\n",
    "import pickle\n",
    "with open('split_dataset/datset70.pickle', 'wb') as f:\n",
    "    pickle.dump([X_train, X_test, y_train, y_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab8b99-9747-43f1-be02-6d1b2d05f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuka dataset yg telah di panggil di file local\n",
    "# with open('pengujian/final/split_dataset/datset60.pickle', 'rb') as f:\n",
    "#     X_train80, X_test80, y_train80, y_test80 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c588958-6ba1-4ccb-986c-23c49358072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Glove\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('dataset/glove/vectors.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433930ee-7c91-46b7-8fb1-ccd23b4bf081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GloVe as Word Embedding\n",
    "hits = 0\n",
    "misses = 0\n",
    "index = []\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "        index.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df46f6-1835-4504-8591-fa218dc925fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = embeddings_index.get(\"netizen\")\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b51025-22d4-4daf-be91-8204ad681a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jumlah kata yg berhasil di embedding\n",
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ece793-056c-415f-bcf9-6c8d94a2978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jumlah kata yg miss tidak ada dalam vector glove\n",
    "misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fd8a7-1bf2-47ed-9c7d-51420134a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kata-kataa yang tidak berhasil di bobot\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28123f61-978a-414d-aba9-65f158e5ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer with glove\n",
    "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCES_LENGTH,weights=[embedding_matrix],trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d1437-b661-4e8d-a45a-609a6fd8070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model CNN\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(16,activation=\"relu\"))\n",
    "#     model.add(Dropout(0.3))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08853161-e1dd-4ccb-8734-5575f836ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c633864-1735-4c6f-b81e-fe40ec6c8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model CNN\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
    "#     model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dense(32,activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de104f-ec3f-489f-b0d9-fd7322ada82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "cnn_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1744d7-c9a7-4040-b08f-4f3d0d7154b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_history = cnn_model.fit(X_train, y_train, epochs=40, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f187d5-14a3-4b77-a1d7-4ec997a15877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisasi model\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "# plot_model(cnn_model, to_file='model1.png', show_shapes=True, show_dtype=False, show_layer_names=True, rankdir='TB', expand_nested=True, ddpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7791ff6-f0d2-4ea6-b74a-10b677086602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model cnn\n",
    "# cnn_model.save('model70_e40.h5')\n",
    "\n",
    "# load model\n",
    "# from keras.models import load_model\n",
    "\n",
    "# cnn_model = load_model('pengujian/model70_e40.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56faddfc-5653-417a-bbfd-1957d7c428d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benerin dulu\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "ypred = cnn_model.predict(X_test)\n",
    "cnn_accuracy = accuracy_score(y_test.argmax(axis=-1),ypred.argmax(axis=-1))\n",
    "print(\"CNN Accuracy:\",cnn_accuracy)\n",
    "cnn_cn = confusion_matrix(y_test.argmax(axis=-1),ypred.argmax(axis=-1))\n",
    "plt.subplots(figsize=(20,16))\n",
    "sns.heatmap(cnn_cn,annot=True,fmt=\"1d\",cmap=\"Blues\",cbar=True,xticklabels=classes,yticklabels=classes)\n",
    "plt.xlabel(\"Predicted\",fontsize=15)\n",
    "plt.ylabel(\"Actual\",fontsize=15)\n",
    "# plt.savefig('model60_e40.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d9be0-45d8-498f-97a0-14e6c312d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5455609-fd0c-4cd5-b7bc-b82301454759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test.argmax(axis=-1), ypred.argmax(axis=-1),target_names= classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466aa04-54ed-4fbf-8ea9-4cc623f57628",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axe1 = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "axe1[0].plot(cnn_history.history[\"accuracy\"],label=\"accuracy\")\n",
    "axe1[0].plot(cnn_history.history[\"val_accuracy\"],label=\"accuracy\")\n",
    "axe1[0].legend(['train', 'val'], loc='upper left')\n",
    "axe1[1].plot(cnn_history.history[\"loss\"],label=\"loss\")\n",
    "axe1[1].plot(cnn_history.history[\"val_loss\"],label=\"loss\")\n",
    "axe1[1].legend(['train', 'val'], loc='upper left')\n",
    "axe1[0].title.set_text(\"CNN Accuracy\")\n",
    "axe1[1].title.set_text(\"CNN Loss\")\n",
    "axe1[0].set_xlabel(\"Epoch\")\n",
    "axe1[1].set_xlabel(\"Epoch\")\n",
    "axe1[0].set_ylabel(\"Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c728a9-47e4-42f6-a47f-a690649d717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08597eee-e635-4d4d-9ac3-7b83c3707d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
